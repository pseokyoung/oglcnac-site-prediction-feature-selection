{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dc03987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# function ConnectButton(){\n",
    "#     console.log(\"Connect pushed\");\n",
    "#     document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
    "# }\n",
    "\n",
    "# setInterval(ConnectButton,60000);\n",
    "# '''\n",
    "\n",
    "# from google.colab import drive\n",
    "# from os import chdir\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# project_path = '/content/drive/MyDrive/Gproject/o-linked-site-prediction-feature-augment'\n",
    "# chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b246081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# set seed for the reproducible result\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41490bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of initial proteins: 104\n",
      "['A2ABU4', 'A2AHJ4', 'A2AKB9', 'A2AQ25', 'E9Q1P8', 'E9Q5G3', 'O08537', 'O09061', 'O35303', 'O70263']\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/integrated_features' # we will get names from the augmented proteins\n",
    "protein_names = [x.split('.')[0] for x in os.listdir(data_dir) if x.split('.')[1] == 'csv'] # get protein name list to be processed for building machine learning models\n",
    "print('the number of initial proteins:', len(protein_names))\n",
    "print(protein_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55978831",
   "metadata": {},
   "source": [
    "## hyper parameter optimization by K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71be2f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from ml_models import *\n",
    "\n",
    "epochs = 1000\n",
    "from keras.callbacks import EarlyStopping\n",
    "patience = 30\n",
    "callbacks = [EarlyStopping(patience=patience, restore_best_weights=True, monitor='val_loss')]\n",
    "\n",
    "test_size = 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4278b",
   "metadata": {},
   "source": [
    "### set initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99162761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7789ffcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial parameters\n",
      "rnn_layers     : 5\n",
      "rnn_neurons    : 64\n",
      "dnn_layers     : 3\n",
      "dnn_neurons    : 64\n",
      "activation     : softmax\n",
      "loss           : categorical_crossentropy\n",
      "metrics        : accuracy\n",
      "optimizer_type : Adam\n",
      "learning_rate  : 0.001\n",
      "regularizer    : {'input': None, 'hidden': None, 'bias': None}\n",
      "window_size    : 10\n"
     ]
    }
   ],
   "source": [
    "initial_params = default_params.copy()\n",
    "initial_params.update({\n",
    "    'window_size'    : 10,\n",
    "    'rnn_layers'     : 5,\n",
    "    'rnn_neurons'    : 64,\n",
    "    'dnn_layers'     : 3,\n",
    "    'dnn_neurons'    : 64\n",
    "    })\n",
    "\n",
    "print('initial parameters')\n",
    "for key, value in initial_params.items():\n",
    "    print(f'{key:<14} : {value}')\n",
    "    \n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "207e69e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['number_hydrophobic_0A',\n",
       " 'number_hydrophilic_0A',\n",
       " 'number_polar_0A',\n",
       " 'number_aromatic_0A',\n",
       " 'number_aliphatic_0A',\n",
       " 'number_charged_0A',\n",
       " 'number_positive_0A',\n",
       " 'number_negative_0A',\n",
       " 'number_gly_0A',\n",
       " 'number_very_small_0A']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_splits = 5\n",
    "\n",
    "regularizer_list = [\n",
    "    {'input': None, 'hidden': None, 'bias': None},\n",
    "    \n",
    "    {'input': 'L21_0.00001', 'hidden': None,          'bias': None},\n",
    "    {'input': 'L21_0.00001', 'hidden': 'L21_0.00001', 'bias': None},\n",
    "    {'input':  'L1_0.00001', 'hidden': None,          'bias': None},\n",
    "    {'input':  'L1_0.00001',  'hidden': 'L1_0.00001', 'bias': None},\n",
    "    \n",
    "    {'input': 'L21_0.0001', 'hidden': None,          'bias': None},\n",
    "    {'input': 'L21_0.0001', 'hidden': 'L21_0.0001', 'bias': None},\n",
    "    {'input':  'L1_0.0001', 'hidden': None,          'bias': None},\n",
    "    {'input':  'L1_0.0001',  'hidden': 'L1_0.0001', 'bias': None},\n",
    "]\n",
    "weights_list = []\n",
    "for regularizer in regularizer_list:\n",
    "    weights_list.append(pd.read_csv(f'./weights/SLSTM_UP_AUGMENT_ONLY_5_64_3_64_Adam_0.001_{regularizer.get(\"input\")}_{regularizer.get(\"hidden\")}_None_10_(11528 21 498).csv',\n",
    "                                index_col=0, names=['weights'], header = 0))\n",
    "print(len(list(weights_list[0].index)))\n",
    "display(list(weights_list[0].index)[:10])\n",
    "display(list(weights_list[0].index)[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b19d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list = [0.05, 0.10, 0.20, 0.40]\n",
    "\n",
    "model_update  = False\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "verbose = 0\n",
    "\n",
    "METRICs = []\n",
    "\n",
    "for i, regularizer in enumerate(regularizer_list):\n",
    "    for i, ratio in enumerate(ratio_list):\n",
    "        for j, top_bottom in enumerate(['TOP', 'BOTTOM']):\n",
    "            model_id = i * 10 + j + 1\n",
    "            \n",
    "            top_weights = pd.concat([weights.sort_values(by = 'weights', ascending = False)[:int(len(weights)*ratio)] for weights in weights_list], axis=1)    \n",
    "            weight_scores = (top_weights > 0).sum(axis=1).sort_values(ascending=False)\n",
    "            display(weight_scores[weight_scores >= min_score])\n",
    "\n",
    "            best_features = list(weight_scores[weight_scores >= min_score].index)\n",
    "            not_in_best   = [x for x in list(weights_list[0].index) if x not in best_features]\n",
    "\n",
    "            model_type = f'{top_bottom}_{ratio}'\n",
    "\n",
    "            # set input features\n",
    "            x_cts = best_features if top_bottom == 'TOP' else not_in_best\n",
    "            x_cat = []\n",
    "            x_var = x_cts + x_cat\n",
    "\n",
    "            # set output feature\n",
    "            y_cts = []\n",
    "            y_cat = ['positivity']\n",
    "            y_var = y_cts + y_cat\n",
    "\n",
    "            params = initial_params.copy()\n",
    "\n",
    "            data_x = []\n",
    "            data_y = []\n",
    "            for name in protein_names:\n",
    "                data = pd.read_csv(f'./data/integrated_features/{name}.csv')\n",
    "                ST_idx = np.where((data['residue'] == 'S') | (data['residue'] == 'T'))[0]\n",
    "\n",
    "                # get X dataset\n",
    "                x_onehot = get_onehots(data[x_var], columns = x_cat)\n",
    "                x_features = list(x_onehot.columns)\n",
    "\n",
    "                # get Y dataset\n",
    "                y_onehot = get_onehots(data[y_var], columns = y_cat)\n",
    "                y_labels = list(y_onehot.columns)\n",
    "\n",
    "                for idx in ST_idx:\n",
    "                    window_x = np.array(get_window(x_onehot, idx, params['window_size']))\n",
    "                    label_y  = np.array(y_onehot.iloc[idx])\n",
    "\n",
    "                    data_x.append(window_x)\n",
    "                    data_y.append(label_y)\n",
    "\n",
    "            data_x = np.array(data_x)\n",
    "            data_y = np.array(data_y)\n",
    "\n",
    "            for cv_idx in range(n_splits):\n",
    "                clear_output(wait=True)\n",
    "                if METRICs:\n",
    "                    display(pd.concat(METRICs).drop(['input', 'hidden'], axis=1).groupby('model_id').mean())\n",
    "                else:\n",
    "                    display(METRICs)\n",
    "\n",
    "                print('data x shape:', data_x.shape)\n",
    "                print(params)\n",
    "\n",
    "                model = LSTM_CLS(data_x.shape[1], data_x.shape[-1], data_y.shape[-1], params) # I don't know why, but this row is helping producing the same training result of a neural network\n",
    "\n",
    "                splitter = StratifiedShuffleSplit(n_splits = n_splits, test_size = test_size, random_state = SEED)\n",
    "                train_idx, test_idx = list(splitter.split(data_x, data_y))[cv_idx]\n",
    "\n",
    "                train_x = data_x[train_idx]\n",
    "                train_y = data_y[train_idx]\n",
    "\n",
    "                test_x = data_x[test_idx]\n",
    "                test_y = data_y[test_idx]\n",
    "\n",
    "                train_x, train_y = upsample_data(train_x, train_y) # up-sample the training dataset\n",
    "                train_x, test_x = data_scaling(train_x, test_x)\n",
    "\n",
    "                model = LSTM_CLS(data_x.shape[1], data_x.shape[-1], data_y.shape[-1], params)\n",
    "                model_name = name_model(f'{model_type}', params)\n",
    "                print('model_name:', model_name)\n",
    "\n",
    "                model_folder  = f'./models/{model_name}_{data_x.shape}'\n",
    "                if not os.path.exists(model_folder):\n",
    "                    os.makedirs(model_folder)\n",
    "                model_path    = f'{model_folder}/{cv_idx}.h5'\n",
    "                metric_path   = f'{model_folder}/{cv_idx}.csv'\n",
    "\n",
    "                if not os.path.exists(model_path) or model_update:\n",
    "                    time_start = time.time()\n",
    "                    history = model.fit(train_x, train_y, verbose=verbose,\n",
    "                                        epochs = 10000, callbacks = callbacks,\n",
    "                                        validation_split = test_size/(1-test_size))\n",
    "                    time_end = time.time()\n",
    "                    training_time = round((time_end - time_start)/60, 3)\n",
    "\n",
    "                    model.save_weights(model_path)\n",
    "\n",
    "                    test_loss, accuracy, precision, recall, f1 = metrics_classification(model, test_x, test_y)\n",
    "                    model_metrics = {\n",
    "                        'model_id' : model_id,\n",
    "                        'cv_idx'   : cv_idx,\n",
    "                        **{f'train_{x}': train_x.shape[x] for x in range(len(train_x.shape))},\n",
    "                        'train_y'     : train_y.shape[-1],\n",
    "                        'test_size'   : test_x.shape[0],\n",
    "                        **params,\n",
    "                        'regularizer_input' : params['regularizer']['input'],\n",
    "                        'regularizer_hidden' : params['regularizer']['hidden'],\n",
    "                        'regularizer_bias' : params['regularizer']['bias'],\n",
    "                        'training_time': training_time,\n",
    "                        'test_loss': test_loss,\n",
    "                        'accuracy': accuracy,\n",
    "                        **{f'precision_{x}': precision[x] for x in range(len(precision))},\n",
    "                        **{f'recall_{x}'   : recall[x] for x in range(len(recall))},\n",
    "                        **{f'f1_{x}'       : f1[x] for x in range(len(f1))}}\n",
    "                    model_metrics = pd.DataFrame([model_metrics]).drop(['activation', 'loss', 'metrics', 'optimizer_type', 'regularizer'], axis=1)\n",
    "                    model_metrics.to_csv(metric_path, index=False)\n",
    "\n",
    "                else:\n",
    "                    model.load_weights(model_path)\n",
    "                    model_metrics = pd.read_csv(metric_path, header=0)\n",
    "                    model_metrics['model_id'] = model_id\n",
    "\n",
    "                print(f'f1 score: {model_metrics.f1_1[0]}')\n",
    "\n",
    "                model_metrics['ratio'] = ratio\n",
    "                METRICs.append(model_metrics[['model_id', 'cv_idx', 'input', 'hidden',\n",
    "                                                'train_2', 'rnn_layers', 'dnn_layers',\n",
    "                                                'f1_1', 'precision_1', 'recall_1', 'training_time', 'test_loss', 'accuracy']])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1812b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_idx</th>\n",
       "      <th>train_0</th>\n",
       "      <th>train_1</th>\n",
       "      <th>train_2</th>\n",
       "      <th>train_y</th>\n",
       "      <th>test_size</th>\n",
       "      <th>rnn_layers</th>\n",
       "      <th>rnn_neurons</th>\n",
       "      <th>dnn_layers</th>\n",
       "      <th>dnn_neurons</th>\n",
       "      <th>...</th>\n",
       "      <th>window_size</th>\n",
       "      <th>training_time</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.9510</td>\n",
       "      <td>0.378750</td>\n",
       "      <td>87.442</td>\n",
       "      <td>97.156</td>\n",
       "      <td>10.802</td>\n",
       "      <td>89.586</td>\n",
       "      <td>32.094</td>\n",
       "      <td>93.178</td>\n",
       "      <td>15.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.6398</td>\n",
       "      <td>0.317314</td>\n",
       "      <td>90.400</td>\n",
       "      <td>97.130</td>\n",
       "      <td>16.892</td>\n",
       "      <td>92.776</td>\n",
       "      <td>29.070</td>\n",
       "      <td>94.804</td>\n",
       "      <td>20.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2306.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0984</td>\n",
       "      <td>0.273853</td>\n",
       "      <td>94.006</td>\n",
       "      <td>96.984</td>\n",
       "      <td>22.050</td>\n",
       "      <td>96.784</td>\n",
       "      <td>22.326</td>\n",
       "      <td>96.880</td>\n",
       "      <td>21.662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cv_idx  train_0  train_1  train_2  train_y  test_size  rnn_layers  \\\n",
       "model_id                                                                      \n",
       "1            2.0   9222.0     21.0     18.0      2.0     2306.0         2.0   \n",
       "2            2.0   9222.0     21.0     18.0      2.0     2306.0         3.0   \n",
       "3            2.0   9222.0     21.0     18.0      2.0     2306.0         4.0   \n",
       "\n",
       "          rnn_neurons  dnn_layers  dnn_neurons  ...  window_size  \\\n",
       "model_id                                        ...                \n",
       "1                64.0         3.0         64.0  ...         10.0   \n",
       "2                64.0         3.0         64.0  ...         10.0   \n",
       "3                64.0         3.0         64.0  ...         10.0   \n",
       "\n",
       "          training_time  test_loss  accuracy  precision_0  precision_1  \\\n",
       "model_id                                                                 \n",
       "1                3.9510   0.378750    87.442       97.156       10.802   \n",
       "2               12.6398   0.317314    90.400       97.130       16.892   \n",
       "3               18.0984   0.273853    94.006       96.984       22.050   \n",
       "\n",
       "          recall_0  recall_1    f1_0    f1_1  \n",
       "model_id                                      \n",
       "1           89.586    32.094  93.178  15.878  \n",
       "2           92.776    29.070  94.804  20.052  \n",
       "3           96.784    22.326  96.880  21.662  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data x shape: (11528, 21, 18)\n",
      "data y shape: (11528, 2)\n",
      "class y counts: [11100   428]\n",
      "class y ratio: [0.9629 0.0371]\n",
      "f1 score: 22.07\n",
      "f1 score: 22.75\n",
      "f1 score: 22.34\n",
      "f1 score: 25.14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path) \u001b[38;5;129;01mor\u001b[39;00m model_update:\n",
      "\u001b[0;32m    107\u001b[0m     time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;32m--> 108\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x_kf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y_kf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    109\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    110\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_x_kf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_y_kf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    111\u001b[0m     time_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;32m    112\u001b[0m     training_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m((time_end \u001b[38;5;241m-\u001b[39m time_start)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\honsu\\anaconda3\\envs\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n",
      "\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\honsu\\anaconda3\\envs\\python39\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n",
      "\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n",
      "\u001b[0;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n",
      "\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n",
      "\u001b[0;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n",
      "\u001b[0;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n",
      "\u001b[0;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n",
      "\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n",
      "\u001b[0;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\honsu\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\honsu\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n",
      "\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\honsu\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n",
      "\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n",
      "\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n",
      "\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n",
      "\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n",
      "\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\honsu\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "\u001b[0;32m   2451\u001b[0m   (graph_function,\n",
      "\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n",
      "\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\honsu\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n",
      "\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n",
      "\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n",
      "\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n",
      "\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n",
      "\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n",
      "\u001b[0;32m   1863\u001b[0m     args,\n",
      "\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n",
      "\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n",
      "\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\honsu\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n",
      "\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n",
      "\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n",
      "\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n",
      "\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\honsu\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n",
      "\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n",
      "\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_update  = False\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "verbose = 0\n",
    "\n",
    "METRICs = []\n",
    "for cv_idx in range(n_splits):\n",
    "    for i, regularizer in enumerate(regularizer_list):\n",
    "        for j, ratio in enumerate(ratio_list):\n",
    "            clear_output(wait=True)\n",
    "            if METRICs:\n",
    "                display(pd.concat(METRICs, axis=0).groupby('model_id').mean())\n",
    "            else:\n",
    "                display(METRICs)\n",
    "                \n",
    "            model_id = 10 * i + j + 1\n",
    "            \n",
    "            weights = pd.read_csv(f'./weights/SLSTM_UP_AUGMENT_ONLY_5_64_3_64_Adam_0.001_{regularizer.get(\"input\")}_{regularizer.get(\"hidden\")}_None_10_(11528, 21, 498).csv',\n",
    "                                index_col=0, names=['weights'])\n",
    "            selected_features = weights.sort_values(by = 'weights', ascending = False).iloc[ : int(len(weights) * ratio)]\n",
    "            selected_features = [x for x in selected_features.weights.index]\n",
    "\n",
    "            model_type = f'SELECTED_{ratio}_{regularizer.get(\"input\")}_{regularizer.get(\"hidden\")}'\n",
    "            \n",
    "            # set input features\n",
    "            x_cts = selected_features\n",
    "            x_cat = []\n",
    "            x_var = x_cts + x_cat\n",
    "\n",
    "            # set output feature\n",
    "            y_cts = []\n",
    "            y_cat = ['positivity']\n",
    "            y_var = y_cts + y_cat\n",
    "\n",
    "            params = initial_params.copy()\n",
    "\n",
    "            data_x = []\n",
    "            data_y = []\n",
    "            for name in protein_names:\n",
    "                data = pd.read_csv(f'./data/integrated_features/{name}.csv')\n",
    "                ST_idx = np.where((data['residue'] == 'S') | (data['residue'] == 'T'))[0]\n",
    "\n",
    "                # get X dataset\n",
    "                x_onehot = get_onehots(data[x_var], columns = x_cat)\n",
    "                x_features = list(x_onehot.columns)\n",
    "\n",
    "                # get Y dataset\n",
    "                y_onehot = get_onehots(data[y_var], columns = y_cat)\n",
    "                y_labels = list(y_onehot.columns)\n",
    "\n",
    "                for idx in ST_idx:\n",
    "                    window_x = np.array(get_window(x_onehot, idx, params['window_size']))\n",
    "                    label_y  = np.array(y_onehot.iloc[idx])\n",
    "\n",
    "                    data_x.append(window_x)\n",
    "                    data_y.append(label_y)\n",
    "\n",
    "            data_x = np.array(data_x)\n",
    "            data_y = np.array(data_y)\n",
    "\n",
    "            print('data x shape:', data_x.shape)\n",
    "            print(params)\n",
    "\n",
    "            model = LSTM_CLS(data_x.shape[1], data_x.shape[-1], data_y.shape[-1], params) # I don't know why, but this row is helping producing the same training result of a neural network\n",
    "\n",
    "            splitter = StratifiedShuffleSplit(n_splits = n_splits, test_size = test_size, random_state = SEED)\n",
    "            train_idx, test_idx = list(splitter.split(data_x, data_y))[cv_idx]\n",
    "\n",
    "            train_x = data_x[train_idx]\n",
    "            train_y = data_y[train_idx]\n",
    "\n",
    "            test_x = data_x[test_idx]\n",
    "            test_y = data_y[test_idx]\n",
    "\n",
    "            train_x, test_x = data_scaling(train_x, test_x)\n",
    "    \n",
    "            model = LSTM_CLS(data_x.shape[1], data_x.shape[-1], data_y.shape[-1], params)\n",
    "            model_name = name_model(f'{model_type}', params)\n",
    "            print('model_name:', model_name)\n",
    "\n",
    "            model_folder  = f'./models/{model_name}_{data_x.shape}'\n",
    "            if not os.path.exists(model_folder):\n",
    "                os.makedirs(model_folder)\n",
    "            model_path    = f'{model_folder}/{cv_idx}.h5'\n",
    "            metric_path   = f'{model_folder}/{cv_idx}.csv'\n",
    "\n",
    "            if not os.path.exists(model_path) or model_update:\n",
    "                time_start = time.time()\n",
    "                history = model.fit(train_x, train_y, verbose=verbose,\n",
    "                                    epochs = 10000, callbacks = callbacks,\n",
    "                                    validation_split = test_size/(1-test_size))\n",
    "                time_end = time.time()\n",
    "                training_time = round((time_end - time_start)/60, 3)\n",
    "\n",
    "                model.save_weights(model_path)\n",
    "\n",
    "                test_loss, accuracy, precision, recall, f1 = metrics_classification(model, test_x, test_y)\n",
    "                model_metrics = {\n",
    "                    'model_id' : model_id,\n",
    "                    'cv_idx'   : cv_idx,\n",
    "                    **{f'train_{x}': train_x.shape[x] for x in range(len(train_x.shape))},\n",
    "                    'train_y'     : train_y.shape[-1],\n",
    "                    'test_size'   : test_x.shape[0],\n",
    "                    **params,\n",
    "                    'regularizer_input' : params['regularizer']['input'],\n",
    "                    'regularizer_hidden' : params['regularizer']['hidden'],\n",
    "                    'regularizer_bias' : params['regularizer']['bias'],\n",
    "                    'training_time': training_time,\n",
    "                    'test_loss': test_loss,\n",
    "                    'accuracy': accuracy,\n",
    "                    **{f'precision_{x}': precision[x] for x in range(len(precision))},\n",
    "                    **{f'recall_{x}'   : recall[x] for x in range(len(recall))},\n",
    "                    **{f'f1_{x}'       : f1[x] for x in range(len(f1))}}\n",
    "                model_metrics = pd.DataFrame([model_metrics]).drop(['activation', 'loss', 'metrics', 'optimizer_type', 'regularizer'], axis=1)\n",
    "                model_metrics.to_csv(metric_path, index=False)\n",
    "\n",
    "            else:\n",
    "                model.load_weights(model_path)\n",
    "                model_metrics = pd.read_csv(metric_path, header=0)\n",
    "                model_metrics['model_id'] = model_id\n",
    "\n",
    "            print(f'f1 score: {model_metrics.f1_1[0]}')\n",
    "\n",
    "            model_metrics['ratio'] = ratio\n",
    "            model_metrics['input']  = regularizer.get(\"input\", None)\n",
    "            model_metrics['hidden']  = regularizer.get(\"hidden\", None)\n",
    "            METRICs.append(model_metrics[['model_id', 'cv_idx', 'input', 'hidden',\n",
    "                                          'train_2', 'rnn_layers', 'dnn_layers',\n",
    "                                          'f1_1', 'precision_1', 'recall_1', 'training_time', 'test_loss', 'accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73abb578",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_update  = False\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "verbose = 0\n",
    "\n",
    "METRICs = []\n",
    "for cv_idx in range(n_splits):\n",
    "    for i, regularizer in enumerate(regularizer_list):\n",
    "        for j, ratio in enumerate(ratio_list):\n",
    "            clear_output(wait=True)\n",
    "            if METRICs:\n",
    "                display(pd.concat(METRICs, axis=0).groupby('model_id').mean())\n",
    "            else:\n",
    "                display(METRICs)\n",
    "                \n",
    "            model_id = 10 * i + j + 1\n",
    "            \n",
    "            weights = pd.read_csv(f'./weights/SLSTM_UP_AUGMENT_ONLY_5_64_3_64_Adam_0.001_{regularizer.get(\"input\")}_{regularizer.get(\"hidden\")}_None_10_(11528, 21, 498).csv',\n",
    "                                index_col=0, names=['weights'])\n",
    "            selected_features = weights.sort_values(by = 'weights', ascending = True).iloc[ : int(len(weights) * ratio)]\n",
    "            selected_features = [x for x in selected_features.weights.index]\n",
    "\n",
    "            model_type = f'SELECTED_C_{ratio}_{regularizer.get(\"input\")}_{regularizer.get(\"hidden\")}'\n",
    "            \n",
    "            # set input features\n",
    "            x_cts = selected_features\n",
    "            x_cat = []\n",
    "            x_var = x_cts + x_cat\n",
    "\n",
    "            # set output feature\n",
    "            y_cts = []\n",
    "            y_cat = ['positivity']\n",
    "            y_var = y_cts + y_cat\n",
    "\n",
    "            params = initial_params.copy()\n",
    "\n",
    "            data_x = []\n",
    "            data_y = []\n",
    "            for name in protein_names:\n",
    "                data = pd.read_csv(f'./data/integrated_features/{name}.csv')\n",
    "                ST_idx = np.where((data['residue'] == 'S') | (data['residue'] == 'T'))[0]\n",
    "\n",
    "                # get X dataset\n",
    "                x_onehot = get_onehots(data[x_var], columns = x_cat)\n",
    "                x_features = list(x_onehot.columns)\n",
    "\n",
    "                # get Y dataset\n",
    "                y_onehot = get_onehots(data[y_var], columns = y_cat)\n",
    "                y_labels = list(y_onehot.columns)\n",
    "\n",
    "                for idx in ST_idx:\n",
    "                    window_x = np.array(get_window(x_onehot, idx, params['window_size']))\n",
    "                    label_y  = np.array(y_onehot.iloc[idx])\n",
    "\n",
    "                    data_x.append(window_x)\n",
    "                    data_y.append(label_y)\n",
    "\n",
    "            data_x = np.array(data_x)\n",
    "            data_y = np.array(data_y)\n",
    "\n",
    "            print('data x shape:', data_x.shape)\n",
    "            print(params)\n",
    "\n",
    "            model = LSTM_CLS(data_x.shape[1], data_x.shape[-1], data_y.shape[-1], params) # I don't know why, but this row is helping producing the same training result of a neural network\n",
    "\n",
    "            splitter = StratifiedShuffleSplit(n_splits = n_splits, test_size = test_size, random_state = SEED)\n",
    "            train_idx, test_idx = list(splitter.split(data_x, data_y))[cv_idx]\n",
    "\n",
    "            train_x = data_x[train_idx]\n",
    "            train_y = data_y[train_idx]\n",
    "\n",
    "            test_x = data_x[test_idx]\n",
    "            test_y = data_y[test_idx]\n",
    "\n",
    "            train_x, test_x = data_scaling(train_x, test_x)\n",
    "    \n",
    "            model = LSTM_CLS(data_x.shape[1], data_x.shape[-1], data_y.shape[-1], params)\n",
    "            model_name = name_model(f'{model_type}', params)\n",
    "\n",
    "            model_folder  = f'./models/{model_name}_{data_x.shape}'\n",
    "            if not os.path.exists(model_folder):\n",
    "                os.makedirs(model_folder)\n",
    "            model_path    = f'{model_folder}/{cv_idx}.h5'\n",
    "            metric_path   = f'{model_folder}/{cv_idx}.csv'\n",
    "\n",
    "            if not os.path.exists(model_path) or model_update:\n",
    "                time_start = time.time()\n",
    "                history = model.fit(train_x, train_y, verbose=verbose,\n",
    "                                    epochs = 10000, callbacks = callbacks,\n",
    "                                    validation_split = test_size/(1-test_size))\n",
    "                time_end = time.time()\n",
    "                training_time = round((time_end - time_start)/60, 3)\n",
    "\n",
    "                model.save_weights(model_path)\n",
    "\n",
    "                test_loss, accuracy, precision, recall, f1 = metrics_classification(model, test_x, test_y)\n",
    "                model_metrics = {\n",
    "                    'model_id' : model_id,\n",
    "                    'cv_idx'   : cv_idx,\n",
    "                    **{f'train_{x}': train_x.shape[x] for x in range(len(train_x.shape))},\n",
    "                    'train_y'     : train_y.shape[-1],\n",
    "                    'test_size'   : test_x.shape[0],\n",
    "                    **params,\n",
    "                    'regularizer_input' : params['regularizer']['input'],\n",
    "                    'regularizer_hidden' : params['regularizer']['hidden'],\n",
    "                    'regularizer_bias' : params['regularizer']['bias'],\n",
    "                    'training_time': training_time,\n",
    "                    'test_loss': test_loss,\n",
    "                    'accuracy': accuracy,\n",
    "                    **{f'precision_{x}': precision[x] for x in range(len(precision))},\n",
    "                    **{f'recall_{x}'   : recall[x] for x in range(len(recall))},\n",
    "                    **{f'f1_{x}'       : f1[x] for x in range(len(f1))}}\n",
    "                model_metrics = pd.DataFrame([model_metrics]).drop(['activation', 'loss', 'metrics', 'optimizer_type', 'regularizer'], axis=1)\n",
    "                model_metrics.to_csv(metric_path, index=False)\n",
    "\n",
    "            else:\n",
    "                model.load_weights(model_path)\n",
    "                model_metrics = pd.read_csv(metric_path, header=0)\n",
    "                model_metrics['model_id'] = model_id\n",
    "\n",
    "            print(f'f1 score: {model_metrics.f1_1[0]}')\n",
    "\n",
    "            model_metrics['ratio'] = ratio\n",
    "            model_metrics['input']  = regularizer.get(\"input\", None)\n",
    "            model_metrics['hidden']  = regularizer.get(\"hidden\", None)\n",
    "            METRICs.append(model_metrics[['model_id', 'cv_idx', 'input', 'hidden',\n",
    "                                          'train_2', 'rnn_layers', 'dnn_layers',\n",
    "                                          'f1_1', 'precision_1', 'recall_1', 'training_time', 'test_loss', 'accuracy']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
